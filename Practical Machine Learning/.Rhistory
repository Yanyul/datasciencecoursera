par(mfrow=c(2,2))
plot(lm.fit.add)
lm.fit.justx1_add <- lm(y~x1, data = data3)
summary(lm.fit.justx1_add)
#Comment:
#plot
par(mfrow=c(2,2))
plot(lm.fit.justx1_add)
lm.fit.justx2_add <- lm(y~x2, data = data3)
summary(lm.fit.justx2_add)
#Comment:
#plot
par(mfrow=c(2,2))
plot(lm.fit.justx2_add)
library("ISLR")
library("MASS")
library("car")
?Boston
str(Boston)
head(Boston)
varset <- Boston[-1]
SimpleLinearInfo <- matrix(0, nrow = 16, ncol = 4, dimnames = (NULL, c("Name", "F-statistics", "p-value", "β")))
SimpleLinearInfo <- matrix(0, nrow = 16, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "β")))
SimpleLinearInfo
SimpleLinearInfo <- matrix(0, nrow = 16, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
SimpleLinearInfo
par(mfrow=c(4,3))
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 16, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
for ( i in 2:16){
lm.fit <- lm(crim~Boston[i], data = Boston)
}
Boston[2]
dimnames(Boston)
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 15, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 15, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
for ( i in 2:16){
lm.fit <- lm(crim~Boston$zn, data = Boston)
}
Boston$(dimnames(Boston[2][i]))
names(Boston)
names(Boston[1])
type(names(Boston[1]))
class(names(Boston[1]))
Boston$"zn"
for ( i in 1:13){
lm.fit <- lm(crim~Boston$names(Boston[i+1]), data = Boston)
}
for ( i in 1:13){
lm.fit <- lm(crim~Boston$names(Boston[i]), data = Boston)
}
lm(crim~Boston$names(Boston[2])), data = Boston)
lm(Boston$crim~Boston$names(Boston[2]))
lm(crim~Boson$names(Boston[2]), data = Boston)
lm(crim~Boston$names(Boston[2]), data = Boston)
for ( i in 1:13){
lm.fit <- lm(Boston[1]~Boston$names(Boston[i]), data = Boston)
}
head(Boston$names(Boston[2]))
for ( i in 1:13){
lm.fit <- lm(crim~Boston$names(Boston)[i+1], data = Boston)
}
Boston$names(Boston)[2]
names(Boston)[2]
Boston$(names(Boston)[2])
Boston$(names(Boston))[2]
(names(Boston))[2]
Boson$"zn"
Boston$"zn"
for ( i in 1:13){
lm.fit <- lm(crim~Boston$(names(Boston))[i+1], data = Boston)
}
lm(crim~Boston[i+1], data = Boston)
Bostom[2]
Boston[2]
Boston$2
name <- names(Boston)
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
}
name <- names(Boston)[-1]
par(mfrow=c(4,3))
for ( i in name){
j <= 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic
SimpleLinearInfo[j,3] <- anova(lm.fit)$"Pr(>F)"
SimpleLinearInfo[j,4] <- coef(lm.fit)
}
j <= 1
for ( i in name){
j <- 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic
SimpleLinearInfo[j,3] <- anova(lm.fit)$"Pr(>F)"
SimpleLinearInfo[j,4] <- coef(lmfit)
}
summary(lm.fit)$fstatistic
summary(lm.fit)$fstatistic$value
summary(lm.fit)$fstatistic[1]
summary(lm.fit)$fstatistic[1,1]
(summary(lm.fit)$fstatistic)$value
class(summary(lm.fit$fstatistic))
summary(lm.fit)$fstatistic[1][1]
for ( i in name){
j <- 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$"Pr(>F)"
SimpleLinearInfo[j,4] <- coef(lmfit)
}
anova(lm.fit)
anova(lm.fit)$Pr(>F)
anova(lm.fit)$'Pr(>F)'
for ( i in name){
j <- 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lmfit)
}
coef(lm.fit)
i
for ( i in name){
j <- 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
}
SimpleLinearInfo
for ( i in name){
j <- 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j += 1
}
for ( i in name){
j <- 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j = j + 1
}
SimpleLinearInfo
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j = j + 1
}
SimpleLinearInfo
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 13, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
name <- names(Boston)[-1]
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j = j + 1
}
SimpleLinearInfo
name <- names(Boston)[-1]
name
lm.fit<-lm(crim~Boston$"indus", data = Boston)
summary(lm.fit)$fstatistic[1][1]
anova(lm.fit)$'Pr(>F)'[1][1]
coef(lm.fit)[2][1]
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
plot(Boston$i, Boston$i)
j = j + 1
}
SimpleLinearInfo
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
plot(Boston$crim, Boston$i)
j = j + 1
}
SimpleLinearInfo
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 13, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
name <- names(Boston)[-1]
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
plot(Boston$i, Boston$crim)
j = j + 1
}
name
lm.fit <- lm(crim~Boston$"zn", data = Boston)
summary(lm.fit)$fstatistic[1][1]
anova(lm.fit)$'Pr(>F)'[1][1]
coef(lm.fit)[2][1]
for (i in name){}
for (i in name){print i}
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 13, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
name <- names(Boston)[-1]
j <- 1
lm.fit <- lm(crim~Boston$"zn", data = Boston)
SimpleLinearInfo[j,1] <- "zn"
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
SimpleLinearInfo
lm.fit <- lm(crim~Boston$"indus", data = Boston)
SimpleLinearInfo[j+1,1] <- "indus"
SimpleLinearInfo[j+1,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j+1,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j+1,4] <- coef(lm.fit)[2][1]
SimpleLinearInfo
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 13, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
name <- names(Boston)[-1]
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
plot(Boston$i, Boston$crim)
j <- j + 1
}
SimpleLinearInfo
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 1
}
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 1
}
SimpleLinearInfo
name <- list(names(Boston)[-1])
names
name
j <- 1
for ( i in name[c(1,2)]){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 1
}
j <- 1
for ( i in c("zn", "indus", "chas")){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 1
}
SimpleLinearInfo
for ( i in c("zn", "indus", "chas")){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 1
}
SimpleLinearInfo
SimpleLinearInfo <- matrix(0, nrow = 13, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
name <- names(Boston)[-1]
j <- 0
for ( i in c("zn", "indus", "chas")){
j <- j + 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 2
}
SimpleLinearInfo
for ( i in c("zn", "indus", "chas")){
j <- j + 1
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 2
j
}
SimpleLinearInfo
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 13, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
name <- names(Boston)[-1]
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 2
}
par(mfrow=c(4,3))
SimpleLinearInfo <- matrix(0, nrow = 13, ncol = 4, dimnames = list(NULL, c("Name", "F-statistics", "p-value", "beta")))
name <- names(Boston)[-1]
j <- 1
for ( i in name){
lm.fit <- lm(crim~Boston$i, data = Boston)
SimpleLinearInfo[j,1] <- i
SimpleLinearInfo[j,2] <- summary(lm.fit)$fstatistic[1][1]
SimpleLinearInfo[j,3] <- anova(lm.fit)$'Pr(>F)'[1][1]
SimpleLinearInfo[j,4] <- coef(lm.fit)[2][1]
j <- j + 1
}
SimpleLinearInfo
setwd("D:/DATA/coursera/Data Scientist/datasciencecoursera/Practical Machine Learning")
memory.limit(102400)
swirl()
library(swirl)
knitr::opts_chunk$set(echo = TRUE,message = FALSE)
# Defaulting the target outcome as 'B'
test$classe <- as.factor('B')
knitr::opts_chunk$set(echo = TRUE,message = FALSE)
library(caret)
library(ggplot2)
library(GGally)
library(reshape)
library(e1071)
library(parallel)
library(doParallel)
library(plyr)
if(!file.exists("pml-training.csv")){download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")}
if(!file.exists("pml-testing.csv")){download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")}
training <- read.csv("./pml-training.csv",header = T,na.strings=c("NA","#DIV/0!",""))
test <- read.csv("./pml-testing.csv",header = T,na.strings=c("NA","#DIV/0!",""))
set.seed(5027)
idxtrain <- createDataPartition(training$classe,p=0.70,list=FALSE)
subtraining <- training[idxtrain,]
subtest <- training[-idxtrain,]
# Check if the response is Null
sum(is.na(subtraining$classe))
# check for missing values in the entire set
dim(training[!complete.cases(subtraining),])[1]
# Check columns in training set having more than 60% missing values
trainp_missing <- ldply(subtraining,function(x) mean(is.na(x)))
trainc_missing <- trainp_missing[trainp_missing$V1 > 0.60,]
#select the columns that do not match with the excluded columns
colindx <- which(!(names(subtraining) %in% trainc_missing$.id))
subtraining_new <- subtraining[,colindx]
# Check if any NA value is present in the cleaned training/test set
sum(is.na(subtraining_new))
all(colSums(is.na(subtraining_new)) ==0)
nsv <- nearZeroVar(subtraining_new,saveMetrics = TRUE)
subtraining_new <- subtraining_new[,nsv$nzv==FALSE]
library(moments)
#FInd the numeric Indexes
colnumindx <- which(lapply(subtraining_new,class) %in% c("numeric","integer"))
subtraining_skew_check <- subtraining_new[,colnumindx]
skewval <- skewness(subtraining_skew_check)
# % of columns having skewness
(sum(skewval < -1) + sum(skewval >1 )) / ncol(subtraining_new)
colskew1 <- which(skewval < -1)
colskew2 <- which(skewval > 1)
# Skewed columns in the dataset
colskew <- c(colskew1,colskew2)
subtraining_skew <- subtraining_new[,colskew]
preobj <- preProcess(subtraining_skew,method=c('center','scale'))
pre_training <- predict(preobj,subtraining_skew)
subtraining_nonskew <- subtraining_new[,setdiff(names(subtraining_new),names(subtraining_skew))]
subtraining <- cbind(pre_training,subtraining_nonskew)
subtest_skew <- subtest[,names(subtraining_skew)]
test_skew <- test[,names(subtraining_skew)]
# Apply the preprocessing to the sub test set and actual test set
subtest_skew <- predict(preobj,subtest_skew)
test_skew <- predict(preobj,test_skew)
subtest_nonskew <- subtest[,names(subtraining_nonskew)]
test_nonskew <- test[,names(subtraining_nonskew)[c(1:46,48:52)]]
# Merge the skew and non-skewed data for preprocessed subtest and test data set
subtest <- cbind(subtest_skew,subtest_nonskew)
test <- cbind(test_skew,test_nonskew)
library(randomForest)
set.seed(5072)
modrf <- randomForest(classe~.,data=subtraining)
#modrf <- train(classe~.,data = subtraining,method="rf")
plot(modrf)
predrf <- predict(modrf,subtest,type="class")
cmrf <- confusionMatrix(predrf,subtest$classe)
cmrf$overall[1]
library(gbm)
set.seed(5072)
#modbst <- gbm(classe ~.,data=subtraining,distribution = "multinomial",n.trees = 500)
trcontrol <- trainControl(method = "repeatedcv",
number = 5,
repeats = 1)
modbst <- train(classe ~ ., data=subtraining, method = "gbm",
trControl = trcontrol,
verbose = FALSE)
plot(modbst)
predbst <- predict(modbst,subtest)
cmbst <- confusionMatrix(predbst,subtest$classe)
cmbst$overall[1]
# Defaulting the target outcome as 'B'
test$classe <- as.factor('B')
combine <- rbind(subtraining,test)
ncol(subtraining)
ncol(test)
ncol(test_skew)
ncol(test_nonskew)
# Defaulting the target outcome as 'B'
test$classe <- as.factor('B')
combine <- cbind(subtraining,test)
test$classe <- as.factor('B')
test$classe
names(test)
names(subtraning)
subtraining
names(subtraining)
names(test)
subtest_skew <- subtest[,names(subtraining_skew)]
test_skew <- test[,names(subtraining_skew)]
# Apply the preprocessing to the sub test set and actual test set
subtest_skew <- predict(preobj,subtest_skew)
test_skew <- predict(preobj,test_skew)
subtest_nonskew <- subtest[,names(subtraining_nonskew)]
test_nonskew <- test[,names(subtraining_nonskew)[c(1:46,48:53)]]
# Merge the skew and non-skewed data for preprocessed subtest and test data set
subtest <- cbind(subtest_skew,subtest_nonskew)
test <- cbind(test_skew,test_nonskew)
# Defaulting the target outcome as 'B'
test$classe <- as.factor('B')
combine <- rbind(subtraining,test)
ncol(test)
subtest_skew <- subtest[,names(subtraining_skew)]
test_skew <- test[,names(subtraining_skew)]
# Apply the preprocessing to the sub test set and actual test set
subtest_skew <- predict(preobj,subtest_skew)
test_skew <- predict(preobj,test_skew)
subtest_nonskew <- subtest[,names(subtraining_nonskew)]
test_nonskew <- test[,names(subtraining_nonskew)[c(1:46,48:53)]]
# Merge the skew and non-skewed data for preprocessed subtest and test data set
subtest <- cbind(subtest_skew,subtest_nonskew)
test <- cbind(test_skew,test_nonskew)
names(test)
ncol(test_nonskew)
ncol(test_skew)
# Defaulting the target outcome as 'B'
test$classe <- as.factor('B')
combine <- rbind(subtraining,test)
ncol(subtraining)
# Defaulting the target outcome as 'B'
test$classe <- as.factor('B')
combine <- rbind(subtraining[-53],test)
idxtrain <- 1:nrow(subtraining)
xtrain <- combine[idxtrain,]
xtest  <- combine[-idxtrain,]
set.seed(5027)
modrf1 <- randomForest(classe~.,data=xtrain,importance=TRUE,ntree=500)
predtest <- predict(modrf1,xtest,type="class")
predtest
knit2html()
library(knitr)
knit2html()
knit2html
knit2html("Machine Learning Assignment.Rmd")
knitr::knit2html()
rmarkdown::render()
rmarkdown::render("Machine Learning Assignment")
rmarkdown::render("Machine Learning Assignment.Rmd")
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, cache = TRUE)
library(swirl)
library(swirl)
